{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    }
   ],
   "source": [
    "# open-source LLM from Hugging Face\n",
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-large\")\n",
    "llm_out=llm(\"Which is most expensive city in the world?\")\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Write a query template\n",
    "template = \"Which is most {input} city in the world?\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=['input'], )\n",
    "\n",
    "#Format the prompt\n",
    "_input=prompt.format(input=\"expensive\")\n",
    "\n",
    "# Generate the output\n",
    "output = llm(_input)\n",
    "\n",
    "# The response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uranus', 'neptune', 'and jupiter']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Create format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt to request a list\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List three {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Define a query to prompt the model\n",
    "query = \"Planets in the Universe\"\n",
    "\n",
    "# Generate the output\n",
    "output = llm(prompt.format(subject=query))\n",
    "\n",
    "# Parse the output using the parser\n",
    "parsed_result = output_parser.parse(output)\n",
    "\n",
    "# The result is a list of items\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"MachineLearning.pdf\")\n",
    "transcript = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12059028446674347, 0.05278531089425087, -0.059828005731105804, 0.08542026579380035, -0.004797864239662886, -0.08946884423494339, 0.01297463197261095, 0.03041219338774681, -0.026835070922970772, 0.05475061014294624, 0.014216860756278038, -0.13152022659778595, -0.003897628979757428, 0.057085853070020676, -0.03267860785126686, -0.06539640575647354, 0.062207747250795364, -0.06350389122962952, 0.04352044686675072, -0.04667651280760765, -0.01896783895790577, -0.060425225645303726, 0.05154573917388916, 0.01322076003998518, 0.07343100011348724, 0.005899892654269934, -0.007350774481892586, 0.009249459952116013, 0.01441187784075737, -0.020577149465680122, -0.060433536767959595, 0.0007745632319711149, 0.042855340987443924, 0.02764873206615448, 0.05072985962033272, -0.004928809590637684, 0.014492416754364967, -0.01681479625403881, 0.029464807361364365, 0.029767077416181564, 0.013785243965685368, -0.026222797110676765, -0.013363873586058617, -0.07667095959186554, -0.01621812954545021, -0.02753910981118679, 0.027980154380202293, 0.09559764713048935, 0.0021995713468641043, 0.028464816510677338, -0.013678738847374916, 0.048078689724206924, -0.06867174804210663, -0.021837903186678886, -0.007933598943054676, 0.01384019386023283, 0.0035741727333515882, -0.027108443900942802, 0.02275479957461357, 0.05444980040192604, -0.057124484330415726, 0.02755839005112648, -0.04068036377429962, 0.007833939045667648, 0.07421751320362091, -0.016470959410071373, 0.042290665209293365, 0.030570486560463905, -0.053906477987766266, -0.0030733111780136824, 0.07933728396892548, -0.05432925373315811, 0.0012277055066078901, -0.012950504198670387, 0.015291230753064156, -0.00370436767116189, 0.0938057228922844, 0.015454940497875214, -0.06264202296733856, 0.053342245519161224, 0.054149892181158066, -0.06116132810711861, -0.04817124083638191, 0.03701520711183548, 0.03447592630982399, 0.033735282719135284, 0.0022325708996504545, 0.021282624453306198, 0.017429715022444725, -0.014908933080732822, 0.02816726081073284, 0.01486624963581562, -0.07398664206266403, 0.023061543703079224, -0.039946604520082474, 0.0817440003156662, -0.029204111546278, 0.03422079607844353, -0.039270658046007156, 0.00273426272906363, 0.000898936705198139, 0.014586960896849632, 0.10636810958385468, 0.05065010115504265, 0.00024339758965652436, -0.03450760990381241, 0.014143615029752254, -0.004870370961725712, -0.052684057503938675, 0.023807397112250328, -0.07248376309871674, -0.04757285490632057, -0.05437949672341347, -0.03319108858704567, -0.04365825653076172, 0.01930118352174759, 0.06799919158220291, -0.03758324682712555, -0.006510636303573847, 0.00924466922879219, -0.004713756497949362, -0.09159807115793228, -0.046985458582639694, 0.03177398815751076, -0.07341764867305756, -0.07267807424068451, -0.04942978918552399, -4.726789884983101e-33, -0.05542536452412605, -0.07102490961551666, 0.05785880610346794, -0.01300437655299902, -0.09296303242444992, -0.03266291320323944, -0.013681227341294289, -0.006839019246399403, -0.021326201036572456, -0.03499831631779671, 0.020210694521665573, -0.09699936956167221, -0.01565573923289776, -0.023846616968512535, 0.13282303512096405, 0.03129127621650696, -5.603775571216829e-05, 0.004097879398614168, -0.1056101843714714, 0.0072352210991084576, 0.004318267572671175, 0.07007358223199844, 0.0555403046309948, -0.03283841535449028, -0.03921559453010559, -0.027978263795375824, 0.03897019848227501, -0.04191778600215912, 0.1020178347826004, -0.008563445881009102, 0.023717928677797318, 0.029385052621364594, 0.06254404038190842, -0.008927092887461185, -0.014742865227162838, 0.06980565190315247, -0.020347120240330696, -0.009751757606863976, -0.05785282701253891, 0.03958519175648689, -0.04205893352627754, -0.04065248742699623, -0.0026684384793043137, 0.03029506839811802, 0.07291954010725021, 0.048956457525491714, -0.040163733065128326, -0.03369954228401184, -0.018043221905827522, -0.08209187537431717, -0.017847275361418724, 0.01832021400332451, -0.13897362351417542, 0.013002059422433376, 0.033305827528238297, -0.02039138786494732, 0.01805693842470646, -0.028267396613955498, 0.017083758488297462, 0.0914691686630249, -0.12384077161550522, 0.01652945578098297, 0.029454803094267845, 0.05017383396625519, 0.05839255079627037, 0.012779820710420609, 0.016940869390964508, 0.006962204817682505, -0.04679565876722336, 0.04710007458925247, 0.009917782619595528, 0.036562949419021606, 0.10243449360132217, 0.047741957008838654, 0.016817089170217514, 0.05922604352235794, 0.03337972238659859, 0.046323370188474655, 0.028133366256952286, 0.044609032571315765, -0.05386333540081978, 0.045677300542593, -0.03774683550000191, 0.06831374764442444, 0.07599099725484848, 0.0047739604488015175, -0.022155607119202614, -0.06525179743766785, 0.015318479388952255, -0.004245841410011053, -0.057813625782728195, -0.003476910525932908, -0.00996518786996603, -0.0871831476688385, -0.02286694198846817, 2.6329914494457726e-33, 0.008660167455673218, -0.026753203943371773, 0.07053632289171219, 0.04549103230237961, -0.02008765935897827, -0.016858365386724472, 0.0020034287590533495, 0.01792474463582039, 0.029350046068429947, 0.09537992626428604, -0.08086295425891876, -0.011546156369149685, 0.11374157667160034, -0.012229707092046738, 0.06533791869878769, 0.0408984012901783, 0.10932998359203339, -0.04347388446331024, -0.05652811378240585, -0.03537739813327789, -0.05042093247175217, 0.03913848102092743, 0.00011950368934776634, 0.025006216019392014, -0.0927223488688469, 0.013636275194585323, -0.12986859679222107, -0.03770660609006882, -0.0369691476225853, -0.02835884876549244, -0.06130165234208107, 0.05012813210487366, -0.06688230484724045, 0.04296664893627167, -0.04059777408838272, 0.10735876858234406, -0.002130344742909074, 0.016694949939846992, 0.002216147491708398, 0.0866815522313118, -0.01593322493135929, -0.003895863890647888, 0.0027011027559638023, 0.0745960995554924, 0.0521196685731411, -0.05443102493882179, -0.12676848471164703, -0.03229115530848503, 0.09315546602010727, -0.059978585690259933, 0.04316253960132599, 0.05843684449791908, -0.08262696117162704, 0.01186755858361721, -0.006282866932451725, 0.04558707773685455, -0.04213680699467659, 0.11623111367225647, -0.011010590940713882, -0.05158436298370361, 0.05325290560722351, 0.046778153628110886, -0.028557412326335907, 0.10840842872858047, -0.04506935924291611, 0.03385316953063011, 0.053518351167440414, 0.023602373898029327, -0.00616368418559432, -0.07035619765520096, 0.011547552421689034, 0.056065693497657776, 0.0004215909284539521, 0.002742120763286948, -0.06695055216550827, 0.04792414978146553, 0.0621972419321537, 0.07434879243373871, 0.12096425145864487, 0.01472035888582468, 0.07833714783191681, 0.03204336017370224, -0.01630084216594696, -0.06067030504345894, -0.03177111595869064, 0.0015053998213261366, 0.046365559101104736, -0.05550876259803772, -0.022859251126646996, -0.007088509853929281, -0.06700816750526428, 0.006875183433294296, 0.0032466298434883356, -0.1276092231273651, -0.018939614295959473, -1.616387024228061e-08, 0.021821744740009308, -0.015182054601609707, -0.02390090562403202, 0.035509273409843445, 0.04861629754304886, -0.06636760383844376, 0.0573786161839962, 0.12731550633907318, -0.010500460863113403, 0.10938042402267456, 0.02632121369242668, 0.03636530414223671, 0.003084128722548485, 0.03187015652656555, -0.13267368078231812, -0.03406887128949165, -0.03324723243713379, -0.001702912151813507, 0.020516712218523026, -0.07118114829063416, -0.0019890854600816965, 0.031718429177999496, -0.013175644911825657, -0.015383458696305752, -0.022799639031291008, -0.04564858600497246, -0.018053395673632622, 0.03467680513858795, 0.0017156461253762245, 0.05857445299625397, 0.040532734245061874, -0.08142176270484924, -0.018373414874076843, -0.04224175587296486, 0.01826327294111252, -0.03692883998155594, -0.011968065053224564, -0.017345961183309555, -0.05366148054599762, -0.08901087939739227, -0.000731367152184248, -0.004086823668330908, -0.029899604618549347, 0.02217656746506691, 0.08037395775318146, -0.055807456374168396, -0.03752615302801132, -0.04341375455260277, 0.0357234962284565, -0.06513352692127228, -0.10967123508453369, 0.03498728573322296, 0.04571590572595596, -0.034230321645736694, -0.01764257252216339, -0.07003472745418549, -0.030622245743870735, -0.00504879467189312, -0.04937373474240303, 0.05254155397415161, 0.08874344080686569, -0.12483584880828857, 0.031913261860609055, 0.0640147253870964]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "text_embeddings = embeddings.embed_query(\"Which is most expensive city in the world?\")\n",
    "print(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure\t1-4.\t\n",
      "Machine\tLearning\tcan\thelp\thumans\tlearn\n",
      "To\tsummarize,\tMachine\tLearning\tis\tgreat\tfor:\n",
      "Problems\tfor\twhich\texisting\tsolutions\trequire\ta\tlot\tof\thand-tuning\tor\tlong\tlists\tof\trules:\tone\tMachine\n",
      "Learning\talgorithm\tcan\toften\tsimplify\tcode\tand\tperform\tbetter.\n",
      "Complex\tproblems\tfor\twhich\tthere\tis\tno\tgood\tsolution\tat\tall\tusing\ta\ttraditional\tapproach:\tthe\tbest\n",
      "Machine\tLearning\ttechniques\tcan\tfind\ta\tsolution.\n",
      "Fluctuating\tenvironments:\ta\tMachine\tLearning\tsystem\tcan\tadapt\tto\tnew\tdata.\n",
      "Getting\tinsights\tabout\tcomplex\tproblems\tand\tlarge\tamounts\t\n",
      "of\tdata.\n"
     ]
    }
   ],
   "source": [
    "query = \"Why Use Machine Learning?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The science (and art) of programming computers so they can learn from data\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retriever = db.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    #return_source_documents=True,\n",
    "    #verbose=True\n",
    ")\n",
    "query = \"What is Machine Learning?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebResearchRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is Machine Learning?', 'answer': '', 'sources': 'Machine Learning is the use of artificial intelligence to learn from data.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"xxx\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"xxx\"\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "web_research_retriever = WebResearchRetriever.from_llm(vectorstore=db, llm=llm, search=search)\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=web_research_retriever,\n",
    "    #return_source_documents=True,\n",
    "    #verbose=True\n",
    ")\n",
    "result = qa_chain({\"question\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a branch of computer science which deals with system programming in order to automatically learn\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader1 = [UnstructuredPDFLoader(\"MLInterview.pdf\") for fn in os.listdir(\"/\")]\n",
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-large\")\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)).from_loaders(loader1)\n",
    "\n",
    "results = index.query(\"What is Machine Learning?\", llm=llm)\n",
    "print(results)\n",
    "\n",
    "#results = index.query(\"\\n When to use ensemble learning?\", llm=llm)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: president of the united states\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=1,\n",
    "                         handle_parsing_errors=True)\n",
    "\n",
    "\n",
    "agent.run(\"Who is Barak Obama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=4pm5C3erORo&pp=ygUVSG93IHRvIGNob29zZSBhIE5pY2hl', 'https://www.youtube.com/watch?v=xXm1Qy9XTYw&pp=ygUVSG93IHRvIGNob29zZSBhIE5pY2hl']\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import YouTubeSearchTool\n",
    "tool = YouTubeSearchTool()\n",
    "\n",
    "ans = tool.run(\n",
    "    \"How to choose a Niche\"\n",
    ")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'president of United States Cite External Websites Also known as: Honest Abe, The Great Emancipator, The Rail-splitter Written by Richard N. Current University Distinguished Professor Emeritus of History, University of North Carolina, Greensboro. Author of Daniel Webster and the Rise of National Conservatism; The Lincoln Nobody Knows; and others. Abraham Lincoln ( / ˈlɪŋkən / LINK-ən; February 12, 1809 - April 15, 1865) was an American lawyer, politician, and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. 1809-1865 Who Was Abraham Lincoln? Abraham Lincoln was the 16 th president of the United States, serving from 1861 to 1865, and is regarded as one of America\\'s greatest heroes due to his... Dec. 20, 2023, 7:12 AM ET (The Hill) Lincoln Memorial steps vandalized with \\'Free Gaza\\' graffiti Lincoln Memorial, stately monument in Washington, D.C., honouring Abraham Lincoln, the 16th president of the United States, and \"the virtues of tolerance, honesty, and constancy in the human spirit.\" 01 Birthplace: Hardin County, Kentucky 02 Birthday: February 12, 1809 03 Death: April 15, 1865 04 Height: 6\\'4 05 Wife: Mary Todd Lincoln 06 Children: Four 07 Political Party: Republican 08 Presidency: 1861-1865 09 Education: Self-educated lawyer 10 Accomplishments: Business development, keeping the nation united, ending slavery Table of Contents'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "search.run(\"Tell me about Abraham Lincoln\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a finance person who likes to roam around the world, and very well versed about the world economy.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "runnable = prompt | llm \n",
    "\n",
    "for chunk in runnable.stream({\"question\": \"Which is most expensive city in the world?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "print(chain.run(question=\"Which is most expensive city in the world?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mlondon\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mUnited Kingdom\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "United Kingdom\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "template1 = \"Which is most {input} city in the world?\"\n",
    "template2 = \"Which is most {input} country in the world?\"\n",
    "\n",
    "prompt1 = PromptTemplate(template=template1, input_variables=['input'], )\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=['input'])\n",
    "\n",
    "chain1 = LLMChain(llm = llm, prompt = prompt1)\n",
    "chain2 = LLMChain(llm = llm, prompt = prompt2)\n",
    "\n",
    "all_chains = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "print(all_chains.run(\"expensive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: I have a cat.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I have a cat.\n",
      "AI: I have a cat.\n",
      "Human: My mom gave me a cat and a dog.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I have a cat.\n",
      "AI: I have a cat.\n",
      "Human: My mom gave me a cat and a dog.\n",
      "AI: I have a dog.\n",
      "Human: Now how many pets I have?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I have three pets.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"I have a cat.\")\n",
    "\n",
    "conversation.predict(input=\"My mom gave me a cat and a dog.\")\n",
    "\n",
    "conversation.predict(input=\"Now how many pets I have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There!\n",
      "AI: Hello!\n",
      "Human: I would like to know more about the world\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human: What would you like to know?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I would like to know more about the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There!\n",
      "AI: Hello!\n",
      "Human: I would like to know more about the world\n",
      "AI: Human: What would you like to know?\n",
      "Human: which is the most populated country in the world?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human: The most populated country in the world is China.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"which is the most populated country in the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi There!'), AIMessage(content='Hello!'), HumanMessage(content='I would like to know more about the world'), AIMessage(content='Human: What would you like to know?'), HumanMessage(content='which is the most populated country in the world?'), AIMessage(content='Human: The most populated country in the world is China.')]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mConversation:: + What is the capital of United States? = \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:03:55.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAnswer:: United States Capitol, Washington, D.C.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Conversation:: + {message} = \")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)\n",
    "msg = chain.run(message=\"What is the capital of United States?\")\n",
    "logger.info(\"Answer:: \" + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
